{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZfH38a-9MKD"
   },
   "source": [
    "# IR Assignment 2\n",
    "\n",
    "## Classification\n",
    "\n",
    "### **Objective**:\n",
    "  - Build classifiers to predict the journal group.\n",
    "\n",
    "### **Algorithms**:\n",
    "  - **Artificial Neural Network (ANN)** (two architectures provided):\n",
    "      - ANN Architecture 1: RELU activation layers.\n",
    "      - ANN Architecture 2: GELU activation layers.\n",
    "  - **Other Classifiers**: Naive Bayes (NB), Support Vector Machine (SVM), Logistic Regression (LoR), Random Forest (RF).\n",
    "\n",
    "### **Tasks**:\n",
    "  - Perform 10-fold cross-validation for all classifiers (except ANN).\n",
    "  - Identify and rank the top 20 most important features for NB, RF, SVM, LoR.\n",
    "  - Write explanations for feature importance in a README document and include the ranked lists in an Excel file.\n",
    "  - Check what is the top 20 most important features for the ANN models.\n",
    "\n",
    "### **ANN Specifics**:\n",
    "  - Split data: Train (80%, with 10% validation from the train set) and Test (20%).\n",
    "  - Use the given ANN architectures with specific configurations:\n",
    "      - Maximum 15 epochs.\n",
    "      - Batch size: 32.\n",
    "      - Early stopping after 3 validation iterations without improvement.\n",
    "      - Save the best model (ModelCheckpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jb5X_3Gb9Hpu"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wFJghx_c9lbB"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/dattali18/IR_Assignments/refs/heads/main/Assignment.01/output/doc2vec/\"\n",
    "\n",
    "file_names = [\"aj\", \"bbc\", \"jp\", \"nyt\"]\n",
    "\n",
    "cluster_map =  {'aj' : 0, 'bbc': 1, 'jp' : 2, 'nyt': 3}\n",
    "\n",
    "links = [f\"{base_url}/{name}_doc2vec.csv\" for name in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkJmeidr9oIG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for name, link in zip(file_names, links):\n",
    "    df = pd.read_csv(link)\n",
    "    # take all the col from 0 - 99 and put them into a numpy array\n",
    "    df_cpy = pd.DataFrame()\n",
    "    df_cpy['vector'] = df.iloc[:, :100].to_numpy().tolist()\n",
    "    df_cpy[\"cluster\"] = str(cluster_map[name])\n",
    "    dfs[name] = df_cpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7w_zIq4E9q1p"
   },
   "outputs": [],
   "source": [
    "# merge all of the df into one df\n",
    "\n",
    "df = pd.concat(dfs.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7WFaSc4R9rlS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.9614479972007641, 0.6957987565937652, 0.59...\n",
       "1    [-0.9792962849264775, 0.7375863705340073, 0.57...\n",
       "2    [-0.9016816118502328, 0.7442563640622617, 0.59...\n",
       "3    [-1.0004149361896462, 0.7354988385819545, 0.59...\n",
       "4    [-0.8834392389517144, 0.8111157942320139, 0.57...\n",
       "Name: std_vector, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standerdize the data mean=0 std=1\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# apply to each line of the df\n",
    "\n",
    "df['std_vector'] = df['vector'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)).flatten())\n",
    "df['std_vector'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data using t-SNE\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "df_tsne = df['std_vector'].apply(lambda x: tsne.fit_transform(np.array(x).reshape(-1, 1)).flatten())\n",
    "df_tsne = pd.DataFrame(df_tsne.to_list(), columns=['x', 'y'])\n",
    "\n",
    "df_tsne['cluster'] = df['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(data=df_tsne, x=\"x\", y=\"y\", hue=\"cluster\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save the data\n",
    "df.to_csv(\"doc2vec_tsne.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FU3jjjrt9vh6"
   },
   "outputs": [],
   "source": [
    "# import all the the needed libraries NaiveBayes, SVM, LoR, RF\n",
    "data = df['std_vector'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2346, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1665, 100)\n",
      "y_train shape: (1665,)\n",
      "X_train type: <class 'numpy.ndarray'>\n",
      "y_train type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# naive bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = data\n",
    "y = df['cluster'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_train type:\", type(X_train))\n",
    "print(\"y_train type:\", type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "# use Naive Bayes with 10-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "scores = cross_val_score(gnb, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results of the classification for all the X\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "plot_confusion_matrix(gnb, X_test, y_test)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the calssification report for all X from the model and color the results using the tsne plot\n",
    "df_tsne['NB_pred'] = gnb.predict(X)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(data=df_tsne, x=\"x\", y=\"y\", hue=\"NB_pred\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 20 features\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n",
    "\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# use SVM with 10-fold cross validation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "scores = cross_val_score(svc, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as NB\n",
    "\n",
    "# visualize the results of the classification for all the X\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "plot_confusion_matrix(svc, X_test, y_test)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the calssification report for all X from the model and color the results using the tsne plot\n",
    "df_tsne[\"SVM_pred\"] = svc.predict(X)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(data=df_tsne, x=\"x\", y=\"y\", hue=\"SVM_pred\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# use Logistic Regression with 10-fold cross validation\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(lr, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same\n",
    "\n",
    "# visualize the results of the classification for all the X\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "plot_confusion_matrix(lr, X_test, y_test)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results using tsne\n",
    "\n",
    "df_tsne[\"LR_pred\"] = lr.predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(data=df_tsne, x=\"x\", y=\"y\", hue=\"LR_pred\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# use Random Forest with 10-fold cross validation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same\n",
    "\n",
    "# visualize the results of the classification for all the X\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "plot_confusion_matrix(rf, X_test, y_test)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results using tsne\n",
    "\n",
    "df_tsne[\"RF_pred\"] = rf.predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(data=df_tsne, x=\"x\", y=\"y\", hue=\"RF_pred\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - Artificial Neural Network Classifier\n",
    "\n",
    "We will build a NN using `tensorflow` and `keras` to classify the journal group.\n",
    "\n",
    "The architecture of the NN is as follows:\n",
    "\n",
    "- Embedding layer with 100 input dimensions.\n",
    "- Hidden layer with 10 node and `relu` activation function.\n",
    "- Hidden layer with 10 node and `relu` activation function.\n",
    "- Hidden layer with 7 node and `relu` activation function.\n",
    "- Output layer with 4 nodes and `softmax` activation function. (4 classes)\n",
    "\n",
    "Seconde architecture:\n",
    "\n",
    "- Embedding layer with 100 input dimensions.\n",
    "- Hidden layer with 10 node and `gelu` activation function.\n",
    "- Hidden layer with 10 node and `gelu` activation function.\n",
    "- Hidden layer with 7 node and `gelu` activation function.\n",
    "- Output layer with 4 nodes and `softmax` activation function. (4 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(100, activation='relu', input_shape=(100,)),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(7, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model_1.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "history = model_1.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "\n",
    "model_2 = Sequential(\n",
    "    [\n",
    "        Dense(100, activation=\"gelu\", input_shape=(100,)),\n",
    "        Dense(10, activation=\"gelu\"),\n",
    "        Dense(10, activation=\"gelu\"),\n",
    "        Dense(7, activation=\"gelu\"),\n",
    "        Dense(4, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "history = model_2.fit(X_train, y_train, epochs=15, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the accuracy of the models\n",
    "\n",
    "# model 1\n",
    "loss, accuracy = model_1.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Model 1 Accuracy: \", accuracy)\n",
    "\n",
    "# model 2\n",
    "\n",
    "loss, accuracy = model_2.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Model 2 Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction for model 1\n",
    "\n",
    "df_tsne[\"model_1_pred\"] = model_1.predict_classes(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(data=df_tsne, x=\"x\", y=\"y\", hue=\"model_1_pred\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction for model 2\n",
    "\n",
    "df_tsne[\"model_2_pred\"] = model_2.predict_classes(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(data=df_tsne, x=\"x\", y=\"y\", hue=\"model_2_pred\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model into a file\n",
    "\n",
    "model_1.save(\"model_1.h5\")\n",
    "\n",
    "model_2.save(\"model_2.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
