{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dattali18/IR_Assignments/blob/main/Assignment.04/IR_04_extract_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN65WG3nm1ZO"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas numpy torch transformers datasets scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the data from the github repository\n",
        "aj_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.01/data/word/A_J_word.csv?raw=true\"\n",
        "bbc_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.01/data/word/BBC_word.csv?raw=true\"\n",
        "jp_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.01/data/word/J_P_word.csv?raw=true\"\n",
        "nyt_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.01/data/word/NYT_word.csv?raw=true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Normalize all types of single and double quotation marks to standard forms\n",
        "    text = re.sub(r\"[‘’`]\", \"'\", text)  # Convert all single quote variations to '\n",
        "    text = re.sub(r\"[“”]\", '\"', text)  # Convert all double quote variations to \"\n",
        "\n",
        "    # remove any and all special characters since it will not be useful for our analysis\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the data\n",
        "aj_df = pd.read_csv(aj_url)\n",
        "bbc_df = pd.read_csv(bbc_url)\n",
        "jp_df = pd.read_csv(jp_url)\n",
        "nyt_df = pd.read_csv(nyt_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# clean the data\n",
        "# aj_df['text'] = aj_df['text'].apply(clean_text)\n",
        "# bbc_df['text'] = bbc_df['text'].apply(clean_text)\n",
        "# jp_df['text'] = jp_df['text'].apply(clean_text)\n",
        "# nyt_df['text'] = nyt_df['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the part you will need the model so I assume there is a variable called `model` which is the calssifier model you trained in the previous part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_all_sentences(df):\n",
        "    # this will return a dict with key the id of the article \"aj_1\" for example\n",
        "    # and a list of all the sentences in the article\n",
        "\n",
        "    all_sentences = {}\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        text = row[\"document\"]\n",
        "        sentences = re.split(r\"[.!?]\", document)\n",
        "        sentences = [sentence for sentence in sentences if sentence != \"\"]\n",
        "        # clean the sentences\n",
        "        sentences = [clean_text(sentence) for sentence in sentences]\n",
        "        all_sentences[row['id']] = sentences\n",
        "\n",
        "    return all_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for each of the dataframes, extract all the sentences\n",
        "\n",
        "aj_sentences = extract_all_sentences(aj_df)\n",
        "bbc_sentences = extract_all_sentences(bbc_df)\n",
        "jp_sentences = extract_all_sentences(jp_df)\n",
        "nyt_sentences = extract_all_sentences(nyt_df)\n",
        "\n",
        "# the sentences are now extracted and cleaned\n",
        "# in the following format {id: [sentence1, sentence2, ...]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# now we wil use the model to classify each sentences for each article\n",
        "# and we will classify the article based on the majority of the sentences\n",
        "\n",
        "# TODO: load the model\n",
        "\n",
        "# df is the new dataframe with id, class1, ..., calss5, majority_class\n",
        "# the classi will contain the number of sentences in the article that belong to that class\n",
        "\n",
        "concat_df = {**aj_sentences, **bbc_sentences, **jp_sentences, **nyt_sentences}\n",
        "\n",
        "df = pd.DataFrame(columns=[\"id\", \"class1\", \"class2\", \"class3\", \"class4\", \"class5\", \"majority_class\"])\n",
        "\n",
        "for article_id, sentences in concat_df.items():\n",
        "    # classify the sentences\n",
        "    # and add 1 to the class of the sentence\n",
        "    pass\n",
        "\n",
        "# for each row in df calculate the majority class\n",
        "# if there is a tie, choose the first\n",
        "for index, row in df.iterrows():\n",
        "    max_class = 0\n",
        "    max_class_count = 0\n",
        "    for i in range(1, 6):\n",
        "        if row[f\"class{i}\"] > max_class_count:\n",
        "            max_class = i\n",
        "            max_class_count = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save df into a csv file for analysis\n",
        "\n",
        "df.to_csv(\"classified_articles.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMb41blo+jYW7lXN51uJQKA",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
