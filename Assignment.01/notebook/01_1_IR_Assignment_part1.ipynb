{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqojJhEM36blY2yLhPYiKN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dattali18/IR_Assignments/blob/main/Assignment.01/notebook/01_1_IR_Assignment_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information Retrival Course\n",
        "\n",
        "## Assigment 1\n",
        "\n",
        "### Step 1: Data Cleaning"
      ],
      "metadata": {
        "id": "HNxp31jPo2Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "qee7aGCkSwJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning the data\n",
        "!rm data.xlsx"
      ],
      "metadata": {
        "id": "M-eSdXEKulXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02f1248-7a2e-4dff-cbd9-a31283c07833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data.xlsx': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloding the data\n",
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.01/data/data.xlsx?raw=true\"\n",
        "output_filename = \"data.xlsx\"\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(output_filename, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "print(f\"The file was downloaded, and it is in {output_filename}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QQT8gAhpH0I",
        "outputId": "69fb04e3-849a-40dd-dc2b-143328792ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file was downloaded, and it is in data.xlsx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_aj = pd.read_excel(output_filename, sheet_name=\"A-J\", engine=\"openpyxl\")\n",
        "data_bbc = pd.read_excel(output_filename, sheet_name=\"BBC\", engine=\"openpyxl\")\n",
        "data_jp = pd.read_excel(output_filename, sheet_name=\"J-P\", engine=\"openpyxl\")\n",
        "data_nyt = pd.read_excel(output_filename, sheet_name=\"NY-T\", engine=\"openpyxl\")"
      ],
      "metadata": {
        "id": "uirPEHNbqkp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.1 - Preaparing the data\n",
        "\n",
        "We need to prepare the data each source at a time due to difference in the naming convenstion."
      ],
      "metadata": {
        "id": "B7ClGE7kCLe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# processing the data for aj\n",
        "\n",
        "col_names_aj = ['title', 'sub_title', 'Body Text']\n",
        "# we will add all the text from the 3 column above (is nan replace by \"\")\n",
        "# we will add a column 'id' that will be aj_<i> where i is the index of the row\n",
        "\n",
        "data_aj = data_aj[col_names_aj]\n",
        "data_aj = data_aj.fillna(\"\")\n",
        "\n",
        "df_aj = pd.DataFrame()\n",
        "df_aj[\"id\"] = range(1, len(data_aj) + 1)\n",
        "df_aj[\"id\"] = \"aj_\" + df_aj[\"id\"].astype(str)\n",
        "df_aj[\"document\"] = data_aj[\"title\"] + \" \" + data_aj[\"sub_title\"] + \" \" + data_aj[\"Body Text\"]\n",
        "\n",
        "print(df_aj.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNF26XL5BG9M",
        "outputId": "652e6888-5d73-4f3d-851b-3c28b509f29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id                                           document\n",
            "0  aj_1  pope renews call for gaza ceasefire, release o...\n",
            "1  aj_2  biden is still the best us president israel co...\n",
            "2  aj_3  israeli air strikes continue across gaza as tr...\n",
            "3  aj_4  police remove pro-palestinian students from pa...\n",
            "4  aj_5  mass graves found at southern gaza hospital ra...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# processing the data for bbc\n",
        "col_names_bbc = ['title', \"Body Text\"]\n",
        "\n",
        "data_bbc = data_bbc[col_names_bbc]\n",
        "data_bbc = data_bbc.fillna(\"\")\n",
        "\n",
        "\n",
        "df_bbc = pd.DataFrame()\n",
        "df_bbc[\"id\"] = range(1, len(data_bbc) + 1)\n",
        "df_bbc[\"id\"] = \"bbc_\" + df_bbc['id'].astype(str)\n",
        "df_bbc[\"document\"] = data_bbc[\"title\"] + \" \" + data_bbc[\"Body Text\"]\n",
        "\n",
        "print(df_bbc.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6w7yKWgCU6T",
        "outputId": "806d7d6c-f61e-456b-b613-474b41bc5683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                           document\n",
            "0  bbc_1  tanks push further into khan younis as israel ...\n",
            "1  bbc_2  kuehne+nagel in milton keynes 'vandalised by p...\n",
            "2  bbc_3  eisenkot: key israeli war leader challenges ne...\n",
            "3  bbc_4  how gunfire and fear engulfed gaza hospital be...\n",
            "4  bbc_5  who fears for remaining patients at gaza's nas...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# processing the data for jp\n",
        "col_names_jp = ['title', \"Body\"]\n",
        "\n",
        "data_jp = data_jp[col_names_jp]\n",
        "data_jp = data_jp.fillna(\"\")\n",
        "\n",
        "df_jp = pd.DataFrame()\n",
        "df_jp[\"id\"] = range(1, len(data_jp) + 1)\n",
        "df_jp[\"id\"] = \"jp_\" + df_jp['id'].astype(str)\n",
        "df_jp[\"document\"] = data_jp[\"title\"] + \" \" + data_jp[\"Body\"]\n",
        "\n",
        "print(df_jp.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRtA0N8DDgxe",
        "outputId": "44726218-85bf-46f1-834a-e442b7be53f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id                                           document\n",
            "0  jp_1  'loved but not lost': piano ensemble presents ...\n",
            "1  jp_2  is jerusalem’s mughrabi bridge in news to spar...\n",
            "2  jp_3  israeli ‘extra-judicial executions' slammed by...\n",
            "3  jp_4  masa israel journey to mark remembrance day wi...\n",
            "4  jp_5  protesters gather across israel, calling for h...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# processing the data for nty\n",
        "col_names_nyt = ['title', 'Body Text']\n",
        "\n",
        "data_nyt = data_nyt[col_names_nyt]\n",
        "data_nyt = data_nyt.fillna(\"\")\n",
        "\n",
        "df_nyt = pd.DataFrame()\n",
        "df_nyt[\"id\"] = range(1, len(data_nyt) + 1)\n",
        "df_nyt[\"id\"] = \"nyt_\" + df_nyt['id'].astype(str)\n",
        "df_nyt[\"document\"] = data_nyt[\"title\"] + \" \" + data_nyt[\"Body Text\"]\n",
        "\n",
        "print(df_nyt.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxdbu91WDtyY",
        "outputId": "e3d19f1b-1189-49bc-eca7-185f12cc5f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                           document\n",
            "0  nyt_1  hezbollah launches rocket barrage after israel...\n",
            "1  nyt_2  gaza offensive to last at least to year’s end,...\n",
            "2  nyt_3  trapped and starving, 2 families in gaza try t...\n",
            "3  nyt_4  u.n. says israel may be restricting gaza aid a...\n",
            "4  nyt_5  help! we’re booked on a nile cruise but worry ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that all the data is in the same format, at data frame of 2 x length of data with 2 column 'id', 'document' and non are empty and contain the concatunation of the title + sub_title + body (if exists)"
      ],
      "metadata": {
        "id": "V8WC-7BAD6LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of document from AJ:\\t {len(df_aj)}')\n",
        "print(f'Number of document from BBC:\\t {len(df_bbc)}')\n",
        "print(f'Number of document from JP:\\t {len(df_jp)}')\n",
        "print(f'Number of document from NYT:\\t {len(df_nyt)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4DGZ_-0EJwk",
        "outputId": "8ac04b58-101a-4c19-fad5-1e3581339942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of document from AJ:\t 599\n",
            "Number of document from BBC:\t 549\n",
            "Number of document from JP:\t 599\n",
            "Number of document from NYT:\t 599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have 4 df with the column of text and title for each one of the 4 source and each data frame has about 600 articles."
      ],
      "metadata": {
        "id": "MUG3H3QivSJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will perform the data cleaning in two steps:\n",
        "\n",
        "1. cleaning the word from all the punctuation marks.\n",
        "  - for example \"How are you?\" -> \"How are you ?\" because \"you\" != \"you?\"\n",
        "2. cleaning the documents with lemmatisation.\n",
        " - \"cleaning\" -> \"clean\"\n",
        "\n",
        " Outpute at the end:\n",
        "\n",
        " 1. 4 cleaned document sets from the ponctuation (not deleting them).\n",
        " 2. 4 cleaned document set by that include only the lemma (not word but their root)."
      ],
      "metadata": {
        "id": "I3bKzqcewN0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.2 - Word"
      ],
      "metadata": {
        "id": "qWml1nPBGrhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# before we perform any cleaning of any text we will do the following:\n",
        "# we will replace all the different kind of '` etc to a standard '\n",
        "# we will replace all the different kind of \" to the standar \"\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  # Normalize all types of single and double quotation marks to standard forms\n",
        "  text = re.sub(r\"[‘’`]\", \"'\", text)  # Convert all single quote variations to '\n",
        "  text = re.sub(r\"[“”]\", '\"', text)   # Convert all double quote variations to \"\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "wQcfsjxpEdiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will clean our data from the hazard\n",
        "\n",
        "df_aj['document'] = df_aj['document'].apply(clean_text)\n",
        "df_bbc['document'] = df_bbc['document'].apply(clean_text)\n",
        "df_jp['document'] = df_jp['document'].apply(clean_text)\n",
        "df_nyt['document'] = df_nyt['document'].apply(clean_text)"
      ],
      "metadata": {
        "id": "gUs9IWkdFIhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performing the cleaning number 1 separeting the words from the punctuation\n",
        "# e.g. 'What is your name?' -> 'What is your name ?'\n",
        "# while \"doesn't\" -> \"doesn't\"\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  # Tokenize with regex to handle punctuation outside of words and contractions\n",
        "  tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b|[^\\w\\s]\", text, re.UNICODE)\n",
        "\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "def clean_df_punctuation(df):\n",
        "  df_copy = df.copy()\n",
        "  df_copy[\"document\"] = df_copy[\"document\"].astype(str).apply(clean_text)\n",
        "  return df_copy"
      ],
      "metadata": {
        "id": "mHQueieTvhyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the cleaning with a df\n",
        "# we can perform more tests as we find extreme cases.\n",
        "\n",
        "text = \"How are you? I'm well thank you. doesn't.\"\n",
        "clean_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iFvD0Yq9yFHN",
        "outputId": "68dbfb60-1ee1-4885-bce3-358425413fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"How are you ? I'm well thank you . doesn't .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean all the data\n",
        "df_aj_word = clean_df_punctuation(df_aj)\n",
        "df_bbc_word = clean_df_punctuation(df_bbc)\n",
        "df_jp_word = clean_df_punctuation(df_jp)\n",
        "df_nyt_word = clean_df_punctuation(df_nyt)"
      ],
      "metadata": {
        "id": "IAu_nHVsyZ5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the first few lines\n",
        "\n",
        "print(df_aj_word['document'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2XPdGH4GYSP",
        "outputId": "d0f3e923-2625-4086-a2be-3bcd2004a3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    pope renews call for gaza ceasefire , release ...\n",
            "1    biden is still the best us president israel co...\n",
            "2    israeli air strikes continue across gaza as tr...\n",
            "3    police remove pro - palestinian students from ...\n",
            "4    mass graves found at southern gaza hospital ra...\n",
            "Name: document, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.3 - Lemmatization\n",
        "\n",
        "Now that we have clean the data in the first type we will do the lemmatisation."
      ],
      "metadata": {
        "id": "nWY9g4jh1GlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing the needed packeges\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bV8XWt5e064W",
        "outputId": "3baae185-9128-4680-e9e2-7df6e9a3cd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qyd_76GHUus",
        "outputId": "b6fb617a-e68f-4fc7-cb3e-fff7edb4044e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# in this part we will:\n",
        "# 1. remove from the data all the 's at the end of word and such\n",
        "# 2. remove all the stop words\n",
        "# 2. lemmatize the for\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def clean_text_lemma(text):\n",
        "  # replace all the I'm to I am etc\n",
        "  text = re.sub(r\"I'm\", \"I am\", text)\n",
        "  text = re.sub(r\"you're\", \"you are\", text)\n",
        "  text = re.sub(r\"he's\", \"he is\", text)\n",
        "  text = re.sub(r\"she's\", \"she is\", text)\n",
        "  text = re.sub(r\"it's\", \"it is\", text)\n",
        "  text = re.sub(r\"we're\", \"we are\", text)\n",
        "  text = re.sub(r\"they're\", \"they are\", text)\n",
        "  text = re.sub(r\"that's\", \"that is\", text)\n",
        "  text = re.sub(r\"what's\", \"what is\", text)\n",
        "  text = re.sub(r\"where's\", \"where is\", text)\n",
        "  text = re.sub(r\"how's\", \"how is\", text)\n",
        "  text = re.sub(r\"i'll\", \"I will\", text)\n",
        "\n",
        "  # remove from the text all the punctatution\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  # tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # remove all the numbers and dates etc\n",
        "  tokens = [word for word in tokens if not any(char.isdigit() for char in word)]\n",
        "\n",
        "  # remove the stopwords\n",
        "  tokens = [word for word in tokens if not word.lower in stop_words]\n",
        "\n",
        "  doc = nlp(' '.join(tokens))\n",
        "  lemmatized_text = ' '.join(token.lemma_ for token in doc)\n",
        "  return lemmatized_text\n",
        "\n",
        "def clean_df_lemma(df):\n",
        "  df_copy = df.copy()\n",
        "  df_copy[\"document\"] = df_copy[\"document\"].astype(str).apply(clean_text_lemma)\n",
        "  return df_copy"
      ],
      "metadata": {
        "id": "dfllz47NGyIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the clean_text_lemma function\n",
        "\n",
        "text = \"How are you? I'm well thank you. doesn't. cleaning busiest 2024 Mon 20nd of June i'll \"\n",
        "print(clean_text_lemma(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cLvRpWuI5Tb",
        "outputId": "37d77910-ab2e-4db0-ea9a-4766060a6dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how be you I be well thank you do not clean busy Mon of June I will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok now that we have a lemmatization function that works even on the extrem cases lets clean the text"
      ],
      "metadata": {
        "id": "y-SOcHcU2kJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the data\n",
        "df_aj_lemma = clean_df_lemma(df_aj)\n",
        "df_bbc_lemma = clean_df_lemma(df_bbc)\n",
        "df_jp_lemma = clean_df_lemma(df_jp)\n",
        "df_nyt_lemma = clean_df_lemma(df_nyt)"
      ],
      "metadata": {
        "id": "AmJ-mA2bMnU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f\"{i + 1}:\\n{df_aj_lemma['document'][i]}\")\n",
        "  print(f\"{df_aj['document'][i]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTM1ANg024om",
        "outputId": "0e8171f2-3797-4947-c8d1-a59e6cc7b217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1:\n",
            "pope renews call for gaza ceasefire release of captive in easter address Pope Francis say his thought go to those face war especially child who have forget how to smile pope francis have renew call for an immediate ceasefire in gaza and the release of all israeli captive in a peacefocuse address mark easter sunday the most important day on the christian calendar\n",
            "pope renews call for gaza ceasefire, release of captives in easter address Pope Francis says his thoughts go to those facing wars, especially children who have 'forgotten how to smile'. pope francis has renewed calls for an immediate ceasefire in gaza and the release of all israeli captives in a peace-focused address marking easter sunday, the most important day on the christian calendar.\n",
            "\n",
            "\n",
            "2:\n",
            "biden be still the good us president israel could wish for the meaningless ceasefire resolution his administration allow the UN Security Council to pass should not fool anyone united states president ronald reagan order to israeli prime minister menachem begin to put an end to his holocaust in lebanon be perhaps the bestknown political anecdote from israel invasion\n",
            "biden is still the best us president israel could wish for The meaningless ceasefire resolution his administration allowed the UN Security Council to pass should not fool anyone. united states president ronald reagan's order to israeli prime minister menachem begin to put an end to his \"holocaust\" in lebanon is perhaps the best-known political anecdote from israel's 1982 invasion.\n",
            "\n",
            "\n",
            "3:\n",
            "israeli air strike continue across gaza as truce talk struggle israeli strike continue to kill Palestinians as PM Netanyahu face pressure from all side over ceasefire proposal the israeli military have launch more air strike across the gaza strip to deadly effect as official discuss the late propose deal to end hostility and exchange prisoner\n",
            "israeli air strikes continue across gaza as truce talks struggle Israeli strikes continue to kill Palestinians as PM Netanyahu faces pressure from all sides over ceasefire proposals. the israeli military has launched more air strikes across the gaza strip to deadly effect as officials discuss the latest proposed deal to end hostilities and exchange prisoners.\n",
            "\n",
            "\n",
            "4:\n",
            "police remove propalestinian student from pariss sciences po university Students stage sitin and hunger strike at the french university leave campus without incident french police officer enter the paris institute of political study sciences po in paris and remove propalestinian student activist who have occupy its building to protest israel war on gaza\n",
            "police remove pro-palestinian students from paris's sciences po university Students staging sit-in and hunger strike at the French university leave campus without incident. french police officers entered the paris institute of political studies (sciences po) in paris and removed pro-palestinian student activists who had occupied its buildings to protest israel's war on gaza.\n",
            "\n",
            "\n",
            "5:\n",
            "mass grave find at southern gaza hospital raid by israeli force search team in Gaza say they ve uncover hundred of body in the ground of Nasser Medical Complex in the city of Khan Younis the hospital be raid by israeli force before their recent withdrawal from the area\n",
            "mass graves found at southern gaza hospital raided by israeli forces Search teams in Gaza say they've uncovered hundreds of bodies in the grounds of Nasser Medical Complex in the city of Khan Younis. The hospital was raided by Israeli forces before their recent withdrawal from the area. \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store the all the cleaned data in files for further usage\n",
        "# the name of the file should be <source>_word.csv with header\n",
        "\n",
        "df_aj_word.to_csv(\"A_J_word.csv\", index=False)\n",
        "df_bbc_word.to_csv(\"BBC_word.csv\", index=False)\n",
        "df_jp_word.to_csv(\"J_P_word.csv\", index=False)\n",
        "df_nyt_word.to_csv(\"NYT_word.csv\", index=False)"
      ],
      "metadata": {
        "id": "OVaf8Axo3Yml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_aj_lemma.to_csv(\"A_J_lemma.csv\", index=False)\n",
        "df_bbc_lemma.to_csv(\"BBC_lemma.csv\", index=False)\n",
        "df_jp_lemma.to_csv(\"J_P_lemma.csv\", index=False)\n",
        "df_nyt_lemma.to_csv(\"NYT_lemma.csv\", index=False)"
      ],
      "metadata": {
        "id": "7ypByc0N5Pxl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}