{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/dattali18/IR_Assignments/blob/main/Assignment.05/IR_05_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# download the files from github\n",
    "\n",
    "pro_israel_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.05/data/Word/pro-israel.txt?raw=true\"\n",
    "pro_palestine_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.05/data/Word/pro-palestine.txt?raw=true\"\n",
    "anti_israel_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.05/data/Word/anti-israel.txt?raw=true\"\n",
    "anti_palestine_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.05/data/Word/anti-palestine.txt?raw=true\"\n",
    "\n",
    "sentences_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.05/data/sentences/sentences.csv?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# download all the file into the same dir as the note book\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# Download files\n",
    "urllib.request.urlretrieve(pro_israel_url, 'pro-israel.txt')\n",
    "urllib.request.urlretrieve(pro_palestine_url, 'pro-palestine.txt')\n",
    "urllib.request.urlretrieve(anti_israel_url, 'anti-israel.txt')\n",
    "urllib.request.urlretrieve(anti_palestine_url, 'anti-palestine.txt')\n",
    "urllib.request.urlretrieve(sentences_url, 'sentences.csv')\n",
    "\n",
    "print(\"Files downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to load and stem words from a file\n",
    "def load_and_stem_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = file.read().splitlines()\n",
    "    stemmed_words = {stemmer.stem(word) for word in words}\n",
    "    return stemmed_words\n",
    "\n",
    "# Load and stem words for each class\n",
    "pro_israel_words = load_and_stem_words('pro-israel.txt')\n",
    "anti_palestine_words = load_and_stem_words('anti-palestine.txt')\n",
    "pro_palestine_words = load_and_stem_words('pro-palestine.txt')\n",
    "anti_israel_words = load_and_stem_words('anti-israel.txt')\n",
    "\n",
    "# Load sentences from the CSV file\n",
    "sentences_df = pd.read_csv('sentences.csv')\n",
    "\n",
    "# Function to classify a sentence\n",
    "def classify_sentence(sentence):\n",
    "    # Tokenize and stem the sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    stemmed_tokens = {stemmer.stem(token) for token in tokens}\n",
    "    \n",
    "    # Check which class the sentence belongs to\n",
    "    if stemmed_tokens & pro_israel_words and not (stemmed_tokens & anti_palestine_words | stemmed_tokens & pro_palestine_words | stemmed_tokens & anti_israel_words):\n",
    "        return 'pro-israel'\n",
    "    elif stemmed_tokens & anti_palestine_words and not (stemmed_tokens & pro_israel_words | stemmed_tokens & pro_palestine_words | stemmed_tokens & anti_israel_words):\n",
    "        return 'anti-palestine'\n",
    "    elif stemmed_tokens & pro_palestine_words and not (stemmed_tokens & pro_israel_words | stemmed_tokens & anti_palestine_words | stemmed_tokens & anti_israel_words):\n",
    "        return 'pro-palestine'\n",
    "    elif stemmed_tokens & anti_israel_words and not (stemmed_tokens & pro_israel_words | stemmed_tokens & anti_palestine_words | stemmed_tokens & pro_palestine_words):\n",
    "        return 'anti-israel'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply classification to each sentence\n",
    "sentences_df['classification'] = sentences_df['sentence'].apply(classify_sentence)\n",
    "\n",
    "# Save classified sentences to separate files\n",
    "for classification in ['pro-israel', 'anti-palestine', 'pro-palestine', 'anti-israel', 'neutral']:\n",
    "    classified_sentences = sentences_df[sentences_df['classification'] == classification]\n",
    "    classified_sentences.to_csv(f'{classification}_classified.csv', index=False)\n",
    "\n",
    "print(\"Classification complete! Check the generated CSV files.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
