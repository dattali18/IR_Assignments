{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Bonus Part 2:\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/dattali18/IR_Assignments/blob/main/Assignment.05/IR_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy torch transformers datasets scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get the data from the github repository\n",
    "aj_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.01/data/word/A_J_word.csv?raw=true\"\n",
    "bbc_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.01/data/word/BBC_word.csv?raw=true\"\n",
    "jp_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.01/data/word/J_P_word.csv?raw=true\"\n",
    "nyt_url = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.01/data/word/NYT_word.csv?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Normalize all types of single and double quotation marks to standard forms\n",
    "    text = re.sub(r\"[‘’`]\", \"'\", text)  # Convert all single quote variations to '\n",
    "    text = re.sub(r\"[“”]\", '\"', text)  # Convert all double quote variations to \"\n",
    "\n",
    "    # remove any and all special characters since it will not be useful for our analysis\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline for sentence segmentation\n",
    "nlp = pipeline(\"sentencizer\")\n",
    "\n",
    "def extract_all_sentences(df):\n",
    "    # this will return a dict with key the id of the article \"aj_1\" for example\n",
    "    # and a list of all the sentences in the article\n",
    "\n",
    "    all_sentences = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[\"document\"]\n",
    "        # Use the pipeline to extract sentences\n",
    "        sentences = nlp(text)\n",
    "        sentences = [sentence['sentence'] for sentence in sentences]\n",
    "        # clean the sentences\n",
    "        sentences = [clean_text(sentence) for sentence in sentences]\n",
    "\n",
    "        # for all sentence in sentences add to df\n",
    "        for sentence in sentences:\n",
    "            all_sentences.append({\"id\": row[\"id\"], \"document\": sentence})\n",
    "\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "aj_sentences = extract_all_sentences(aj_df)\n",
    "bbc_sentences = extract_all_sentences(bbc_df)\n",
    "jp_sentences = extract_all_sentences(jp_df)\n",
    "nyt_sentences = extract_all_sentences(nyt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "aj_df = pd.DataFrame(aj_sentences)\n",
    "bbc_df = pd.DataFrame(bbc_sentences)\n",
    "jp_df = pd.DataFrame(jp_sentences)\n",
    "nyt_df = pd.DataFrame(nyt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"id\", \"document\"])\n",
    "\n",
    "df = pd.concat([df, aj_df], ignore_index=True)\n",
    "df = pd.concat([df, bbc_df], ignore_index=True)\n",
    "df = pd.concat([df, jp_df], ignore_index=True)\n",
    "df = pd.concat([df, nyt_df], ignore_index=True)\n",
    "\n",
    "# rename the document col to sentence\n",
    "\n",
    "df.rename([\"document\"], [\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the sentence into a vector using SBERT and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
