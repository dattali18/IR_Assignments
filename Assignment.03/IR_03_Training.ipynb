{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Assignment 03\n",
    "\n",
    "## Sentiment Analysis \n",
    "\n",
    "In this assignment we are asked to perform sentiment analysis on the article we were given for assignment 01. This mean that in stage 1 we have built a corpus of positive (in our context) and negative words. In stage 2 we will use this corpus to perform sentiment analysis on the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Install the required libraries\n",
    "\n",
    "```bash\n",
    "pip install tensorflow tensorflow_hub tensorflow_text transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "github_link = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.03/extracted_sentences.csv?raw=true\"\n",
    "\n",
    "df = pd.read_csv(github_link)\n",
    "\n",
    "# label mapping\n",
    "label_mapping = { 'pro-israeli': 0, 'pro-palestinian': 1, 'neutral': 2 }\n",
    "df['label_int'] = df['label'].map(label_mapping)\n",
    "\n",
    "# make sure all the labels are in the right format \n",
    "# meaning if some labels didn't get mapped, then we should remove them\n",
    "\n",
    "df = df.dropna(subset=['label_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a subset of the data using random sampling\n",
    "df_subset = df.sample(n=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing 30% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_subset['sentence']\n",
    "y = df_subset['label_int']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# random_state is set to 42 so that the results are reproducible\n",
    "\n",
    "# load the tokenizer\n",
    "# take the smallest model possible since my machine is not very powerful\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_sentences(sentences, tokenizer, max_length):\n",
    "    tokenized_sentences = tokenizer(\n",
    "        list(sentences),\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    return   {\n",
    "            \"input_ids\": tokenized_sentences[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized_sentences[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "# Tokenize the training and testing data\n",
    "max_length = 128\n",
    "X_train_tokenized = tokenize_sentences(X_train, tokenizer, max_length)\n",
    "X_test_tokenized = tokenize_sentences(X_test, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train_tokenized), y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_tokenized), y_test)).batch(batch_size)\n",
    "\n",
    "# load the model, get the smallest model possible\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "EPOCHS = 5\n",
    "history = model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test loss: {loss}\")\n",
    "\n",
    "model_name_save = \"model_1\"\n",
    "# save the model\n",
    "model.save_pretrained(model_name_save)\n",
    "tokenizer.save_pretrained(model_name_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
