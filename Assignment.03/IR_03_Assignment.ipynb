{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "## Assignment 03\n",
    "\n",
    "**context**: in the first assignment we were given a dataset of of articles from 4 sources (AJ, BBC, NYT, J-P) with $\\approx 600$ articles per source.\n",
    "\n",
    "In this assignment we are asked to perform sentiment analysis on the article.\n",
    "\n",
    "We have divided the assignment into stages:\n",
    "\n",
    "1. Data Preprocessing - Tokenization, Stopwords Removal, Stemming (Done in Assignment 02)\n",
    "2. Sentence Extraction - we take a database of words with positive and negative connotation and we extract the sentences that contain these words. This will be the base of our sentiment analysis.\n",
    "3. Sentiment Analysis - we will use the extracted sentences to perform sentiment analysis on the articles. We will use many models from `huggingface` transformers to perform this task. And for each sentence we make the models vote on the sentence wether it is positive or negative. We will then use the majority vote to determine the sentiment of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Sentence Extraction\n",
    "\n",
    "In this section we will perform the sentence extraction.\n",
    "\n",
    "We have two databases of words with positive and negative connotation. We will use these words to extract the sentences that contain these words.\n",
    "\n",
    "Those are the `israel.txt` and `palestine.txt` files. With each about 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_relevant_sentences(df, pro_israeli_words, pro_palestinian_words):\n",
    "    extracted = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = row['id']\n",
    "        document = row['document']\n",
    "\n",
    "        # Split into sentences\n",
    "        sentences = re.split(r'[.!?]', document)  # Basic sentence splitting\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip().lower()\n",
    "            is_pro_israeli = any(word in sentence for word in pro_israeli_words)\n",
    "            is_pro_palestinian = any(word in sentence for word in pro_palestinian_words)\n",
    "\n",
    "            if is_pro_israeli and not is_pro_palestinian:\n",
    "                extracted.append((doc_id, sentence, 'pro-israeli'))\n",
    "            elif is_pro_palestinian and not is_pro_israeli:\n",
    "                extracted.append((doc_id, sentence, 'pro-palestinian'))\n",
    "\n",
    "    return pd.DataFrame(extracted, columns=['id', 'sentence', 'type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the code snippet about, the function `extract_related_sentences` will extract the sentences that contain the words from the database. The idea behind is very simple if the word $w\\in \\text{Israel}, w \\notin \\text{Palestine}$ then the sentence is positive and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code is just loading the files from the GitHub repo and performing the same cleaning steps as in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# remove special characters from the sentences\n",
    "df_results[0][\"sentence\"] = df_results[0][\"sentence\"].apply(clean_text)\n",
    "df_results[1][\"sentence\"] = df_results[1][\"sentence\"].apply(clean_text)\n",
    "df_results[2][\"sentence\"] = df_results[2][\"sentence\"].apply(clean_text)\n",
    "df_results[3][\"sentence\"] = df_results[3][\"sentence\"].apply(clean_text)\n",
    "\n",
    "# Combine results\n",
    "print(\"Combining results\")\n",
    "df_extracted = pd.concat(df_results)\n",
    "df_extracted.to_csv(\"extracted_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in this stage is concatenating the sentences and saving them to a file `extracted_sentences.csv` for the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Sentiment Analysis\n",
    "\n",
    "In this stage we will perform the sentiment analysis on the extracted sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "link = \"https://github.com/dattali18/IR_Assignments/blob/main/Assignment.03/extracted_sentences.csv?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Download your CSV\n",
    "df = pd.read_csv(link)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_sentiment_models():\n",
    "    model_paths = {\n",
    "        'model1': \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "        'model2': \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "        'model3': \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "        'model4': \"siebert/sentiment-roberta-large-english\",\n",
    "        'model5': \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "        'model6': \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "        'model7': \"j-hartmann/sentiment-roberta-large-english-3-classes\"\n",
    "    }\n",
    "\n",
    "    loaded_models = {}\n",
    "    for name, path in model_paths.items():\n",
    "        try:\n",
    "            loaded_models[name] = pipeline(\"sentiment-analysis\", model=path)\n",
    "            print(f\"Loaded {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {name}: {e}\")\n",
    "\n",
    "    return loaded_models\n",
    "\n",
    "# Load models\n",
    "models = load_sentiment_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the code above, we are loading a total of 7 models from `huggingface` transformers. We will use these models to perform the sentiment analysis on the extracted sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def interpret_sentiment(sentiment_label, score, sentence_type):\n",
    "    \"\"\"Convert model sentiment to pro-israeli/pro-palestinian context\"\"\"\n",
    "    if 'neutral' in sentiment_label.lower():\n",
    "        return 'NEUTRAL'\n",
    "\n",
    "    # If sentence is about Israel\n",
    "    if sentence_type == 'pro-israeli':\n",
    "        return 'POS' if sentiment_label.lower() in ['positive', 'pos'] else 'NEG'\n",
    "    # If sentence is about Palestine\n",
    "    elif sentence_type == 'pro-palestinian':\n",
    "        return 'POS' if sentiment_label.lower() in ['positive', 'pos'] else 'NEG'\n",
    "\n",
    "    return 'NEUTRAL'\n",
    "\n",
    "def analyze_sentence(sentence, sentence_type, models):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            prediction = model(sentence)[0]\n",
    "            results[f\"{name}_score\"] = prediction['score']\n",
    "            results[f\"{name}_label\"] = interpret_sentiment(\n",
    "                prediction['label'],\n",
    "                prediction['score'],\n",
    "                sentence_type\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name} on sentence: {sentence[:50]}... Error: {e}\")\n",
    "            results[f\"{name}_score\"] = None\n",
    "            results[f\"{name}_label\"] = None\n",
    "    return results\n",
    "\n",
    "def get_majority_decision(row):\n",
    "    \"\"\"Get the majority decision across all models\"\"\"\n",
    "    labels = [v for k, v in row.items() if '_label' in k and v is not None]\n",
    "    if not labels:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "    from collections import Counter\n",
    "    count = Counter(labels)\n",
    "    return count.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the idea behind is for every sentence we will use all the models to vote on the sentiment of the sentence. We will then use the majority vote to determine the sentiment of the sentence. We will also extract the score + label from each model as instructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Process each sentence\n",
    "results = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    result = {\n",
    "        'newspaper': row['id'].split('_')[0],\n",
    "        'article_id': row['id'],\n",
    "        'sentence': row['sentence'],\n",
    "        'type': row['type']\n",
    "    }\n",
    "\n",
    "    # Add model predictions\n",
    "    result.update(analyze_sentence(row['sentence'], row['type'], models))\n",
    "\n",
    "    # Add to results\n",
    "    results.append(result)\n",
    "\n",
    "# Create final DataFrame\n",
    "output_df = pd.DataFrame(results)\n",
    "\n",
    "# Add majority decision\n",
    "output_df['majority_decision'] = output_df.apply(get_majority_decision, axis=1)\n",
    "\n",
    "# Calculate average score for majority decision\n",
    "score_columns = [col for col in output_df.columns if '_score' in col]\n",
    "output_df['avg_majority_score'] = output_df[score_columns].mean(axis=1)\n",
    "\n",
    "print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "output_df.to_excel('sentiment_analysis_results.xlsx', index=False)\n",
    "\n",
    "# Download the file (in Colab)\n",
    "from google.colab import files\n",
    "files.download('sentiment_analysis_results.xlsx')\n",
    "\n",
    "# Display some summary statistics\n",
    "print(\"\\nSummary of results:\")\n",
    "print(\"\\nMajority decisions distribution:\")\n",
    "print(output_df['majority_decision'].value_counts())\n",
    "print(\"\\nAverage scores by newspaper:\")\n",
    "print(output_df.groupby('newspaper')['avg_majority_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is:\n",
    "\n",
    "```txt\n",
    "Summary of results:\n",
    "\n",
    "Majority decisions distribution:\n",
    "majority_decision\n",
    "NEG        652\n",
    "NEUTRAL    258\n",
    "POS         90\n",
    "Name: count, dtype: int64\n",
    "\n",
    "Average scores by newspaper:\n",
    "newspaper\n",
    "aj     0.754490\n",
    "bbc    0.744012\n",
    "jp     0.740172\n",
    "nyt    0.739272\n",
    "Name: avg_majority_score, dtype: float64\n",
    "```\n",
    "\n",
    "**NOTE** in this context positive mean pro-palestinian and negative means pro-israel meaning there is a clear bias in those articles."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
